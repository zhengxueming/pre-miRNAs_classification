{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723, 164, 5, 1)\n",
      "(401, 164, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "FILE_PATH = \"miRBase_set.csv\" \n",
    "FILE_PATH_PUTATIVE = \"putative_mirtrons_set.csv\"\n",
    "\n",
    "# read csv file and generate the train and test data set\n",
    "def generate_data(file1_path,file2_path):\n",
    "    csv_reader=csv.reader(open(file1_path, encoding='utf-8'))\n",
    "    csv_reader_putative=csv.reader(open(file2_path, encoding='utf-8'))\n",
    "\n",
    "    all_data_set = []\n",
    "    tansformed_all_data_set = []\n",
    "    y_all_data_set = []\n",
    "    \n",
    "    training_x_data=[]\n",
    "    training_y_data=[]\n",
    "    test_x_data=[]\n",
    "    test_y_data=[]\n",
    "\n",
    "#     read the data into a list(name,sequence,class)\n",
    "    for row in csv_reader:\n",
    "        all_data_set.append([row[0],row[1],row[2]])\n",
    "    for row in csv_reader_putative:\n",
    "        all_data_set.append([row[0],row[1],row[2]])    \n",
    "#     shuffle the data set randomly    \n",
    "    random.seed(2)\n",
    "    random.shuffle(all_data_set)\n",
    "\n",
    "#     get the maxmium length of the seqence\n",
    "    max_seq_len = 0\n",
    "    for item in all_data_set:\n",
    "        if len(item[1])>max_seq_len:\n",
    "            max_seq_len = len(item[1])\n",
    "#     print(max_seq_len)\n",
    "    \n",
    "#   序列用N填充到长度都为max_seq_len\n",
    "    for item in all_data_set:\n",
    "        item[1] += \"N\" *(max_seq_len-len(item[1]))\n",
    "\n",
    "    \n",
    "#     tranformation of data set:word embedding,one_hot vector\n",
    "    x_cast = {\"A\":[[0],[1],[0],[0],[0]],\"U\":[[0],[0],[1],[0],[0]],\\\n",
    "              \"T\":[[0],[0],[1],[0],[0]],\"G\":[[0],[0],[0],[1],[0]],\\\n",
    "              \"C\":[[0],[0],[0],[0],[1]],\"N\":[[0],[0],[0],[0],[0]]}\n",
    "    y_cast = {\"TRUE\": [1,0],\"FALSE\":[0,1]} #TRUE:Mirtrons  FALSE:canonical microRN\n",
    "    \n",
    "    for item in all_data_set:\n",
    "        data = []\n",
    "        for char in item[1]:\n",
    "            data.append(x_cast[char])\n",
    "        tansformed_all_data_set.append([item[0],data,y_cast[item[2]]])\n",
    "        \n",
    "    \n",
    "    i = j = 0\n",
    "    for item in tansformed_all_data_set:\n",
    "        if item[2]==[1,0]:\n",
    "            i = i + 1\n",
    "            if i <= 201:\n",
    "                test_x_data.append(item[1])\n",
    "                test_y_data.append(item[2])\n",
    "            else:\n",
    "                training_x_data.append(item[1])\n",
    "                training_y_data.append(item[2])\n",
    "        else:\n",
    "            j = j + 1\n",
    "            if  j<= 200:\n",
    "                test_x_data.append(item[1])\n",
    "                test_y_data.append(item[2])\n",
    "            else:\n",
    "                training_x_data.append(item[1])\n",
    "                training_y_data.append(item[2])\n",
    "\n",
    "    return training_x_data,training_y_data,test_x_data,test_y_data\n",
    "    \n",
    "training_x_data,training_y_data,test_x_data,test_y_data =\\\n",
    "generate_data(FILE_PATH,FILE_PATH_PUTATIVE)\n",
    "\n",
    "training_x_data = np.array(training_x_data)\n",
    "print(training_x_data.shape)\n",
    "\n",
    "test_x_data = np.array(test_x_data)\n",
    "print(test_x_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-43a869a5fb0c>:132: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "The0iteration:\n",
      "The cross_entropy_mean为：0.6296295\n",
      "The accuracy on training data:0.70124483\n",
      "The accuracy on test data:0.49875313\n",
      "==================\n",
      "The100iteration:\n",
      "The cross_entropy_mean为：0.7373719\n",
      "The accuracy on training data:0.70124483\n",
      "The accuracy on test data:0.49875313\n",
      "==================\n",
      "The200iteration:\n",
      "The cross_entropy_mean为：0.24640739\n",
      "The accuracy on training data:0.9142462\n",
      "The accuracy on test data:0.86533666\n",
      "==================\n",
      "The300iteration:\n",
      "The cross_entropy_mean为：0.08803768\n",
      "The accuracy on training data:0.9681881\n",
      "The accuracy on test data:0.8952618\n",
      "==================\n",
      "The400iteration:\n",
      "The cross_entropy_mean为：0.047690015\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.9077307\n",
      "==================\n",
      "The500iteration:\n",
      "The cross_entropy_mean为：0.048659366\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.90274316\n",
      "==================\n",
      "The600iteration:\n",
      "The cross_entropy_mean为：0.055959363\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.9077307\n",
      "==================\n",
      "The700iteration:\n",
      "The cross_entropy_mean为：0.039554667\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.9077307\n",
      "==================\n",
      "The800iteration:\n",
      "The cross_entropy_mean为：0.013455751\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.91022444\n",
      "==================\n",
      "The900iteration:\n",
      "The cross_entropy_mean为：0.004503966\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.9052369\n",
      "==================\n",
      "The1000iteration:\n",
      "The cross_entropy_mean为：0.0036358724\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.915212\n",
      "==================\n",
      "The1100iteration:\n",
      "The cross_entropy_mean为：0.0046166815\n",
      "The accuracy on training data:0.99723375\n",
      "The accuracy on test data:0.9127182\n",
      "==================\n",
      "The1200iteration:\n",
      "The cross_entropy_mean为：0.0061439453\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9077307\n",
      "==================\n",
      "The1300iteration:\n",
      "The cross_entropy_mean为：0.0025752354\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9177057\n",
      "==================\n",
      "The1400iteration:\n",
      "The cross_entropy_mean为：0.025227522\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.8952618\n",
      "==================\n",
      "The1500iteration:\n",
      "The cross_entropy_mean为：0.016814437\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9201995\n",
      "==================\n",
      "The1600iteration:\n",
      "The cross_entropy_mean为：0.024100779\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9127182\n",
      "==================\n",
      "The1700iteration:\n",
      "The cross_entropy_mean为：0.0035842909\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9127182\n",
      "==================\n",
      "The1800iteration:\n",
      "The cross_entropy_mean为：0.00094258686\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9177057\n",
      "==================\n",
      "The1900iteration:\n",
      "The cross_entropy_mean为：0.0020164347\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9177057\n",
      "==================\n",
      "The2000iteration:\n",
      "The cross_entropy_mean为：0.012411272\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9201995\n",
      "==================\n",
      "The2100iteration:\n",
      "The cross_entropy_mean为：0.0051780455\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9201995\n",
      "==================\n",
      "The2200iteration:\n",
      "The cross_entropy_mean为：0.00033027367\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9127182\n",
      "==================\n",
      "The2300iteration:\n",
      "The cross_entropy_mean为：0.0046520233\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9177057\n",
      "==================\n",
      "The2400iteration:\n",
      "The cross_entropy_mean为：0.021509232\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.915212\n",
      "==================\n",
      "The2500iteration:\n",
      "The cross_entropy_mean为：0.018723657\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9127182\n",
      "==================\n",
      "The2600iteration:\n",
      "The cross_entropy_mean为：0.008200057\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9201995\n",
      "==================\n",
      "The2700iteration:\n",
      "The cross_entropy_mean为：0.004020636\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9127182\n",
      "==================\n",
      "The2800iteration:\n",
      "The cross_entropy_mean为：0.0010500749\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9201995\n",
      "==================\n",
      "The2900iteration:\n",
      "The cross_entropy_mean为：3.411101e-05\n",
      "The accuracy on training data:0.9986169\n",
      "The accuracy on test data:0.9077307\n",
      "==================\n",
      "*********training finished********\n",
      "tp:175.0,tn:194.0,fp:6.0,fn:26.0\n",
      "Sensitivity/recall on the test data is :0.8706467661691543\n",
      "specifity on the test data is :0.97\n",
      "precision on the test data is :0.9668508287292817\n",
      "f1_score on the test data is :0.9162303664921466\n",
      "fpr on the test data is :0.029850746268656716\n",
      "mcc on the test data is :0.844648364270423\n",
      "accuracy on the test data is :0.9201995012468828\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# convolution with one filter of size 3\n",
    "# max-pooling over the entire feature map\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "#hyperparameters\n",
    "LR = 0.001       #learning rate\n",
    "TRAINING_ITER = 3000   #iteration times\n",
    "BATCH_SIZE = 180        #batch size of input\n",
    "\n",
    "SEQUENCE_LENGTH = 164   #sequence length of input\n",
    "EMBEDDING_SIZE = 5      #char embedding size(sequence width of input)\n",
    "\n",
    "CONV_SIZE = 6    #first filter size\n",
    "CONV_DEEP = 128   #number of first filter(convolution deepth)\n",
    "\n",
    "STRIDES = [1,1,1,1]  #the strid in each of four dimensions during convolution\n",
    "KSIZE = [1,164,1,1]    #pooling window size\n",
    "\n",
    "FC_SIZE = 128     #nodes of full-connection layer\n",
    "NUM_CLASSES = 2   # classification number\n",
    "\n",
    "DROPOUT_KEEP_PROB = 0.5   #keep probability of dropout\n",
    "dataset_size = len(training_x_data)  #number of training dataset\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#define placeholder\n",
    "input_xs = tf.placeholder(tf.float32,[None,SEQUENCE_LENGTH,EMBEDDING_SIZE,1])\n",
    "input_ys = tf.placeholder(tf.float32,[None, NUM_CLASSES])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# function of initialize weights\n",
    "def get_weights_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.01)\n",
    "    weights = tf.Variable(initial,name = \"weights\")\n",
    "    return weights\n",
    "\n",
    "# function of convolution and pooling\n",
    "def conv_and_pooling(input_tensor,filter_size,filter_width,depth,conv_deep,layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        conv_weights = get_weights_variable\\\n",
    "                    ([filter_size,filter_width,depth,conv_deep])\n",
    "        conv_bias = tf.Variable(tf.constant(0.1,shape=[conv_deep]),name = \"bias\")   \n",
    "        conv = tf.nn.conv2d(input_tensor,conv_weights,strides = STRIDES,padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv,conv_bias))\n",
    "        pool = tf.nn.max_pool(relu,ksize=KSIZE,strides=STRIDES,padding='VALID')\n",
    "#     tensorboard visualization\n",
    "        tf.summary.histogram(\"conv_weights\",conv_weights)\n",
    "        tf.summary.histogram(\"conv_bias\",conv_bias)\n",
    "        return pool\n",
    "\n",
    "# design the CNN structure\n",
    "def cnn_inference():\n",
    "#     layer of convolution and max-pooling\n",
    "    pool = conv_and_pooling(input_xs,CONV_SIZE,EMBEDDING_SIZE,1,CONV_DEEP,\"filter1\") \n",
    "    \n",
    "#     concatenant\n",
    "#     pool = tf.concat([pool],1)\n",
    "#     tf.summary.scalar(\"concate_the_filters\",pool)\n",
    "    # transformation of four-dimension into two-dimension tensor,the first dimension \n",
    "    # is the batch size and the second dimension is from each sequence\n",
    "#     pool_shape = pool3.get_shape().as_list()\n",
    "#     nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "#     reshaped = tf.reshape(pool3,[-1,nodes])\n",
    "\n",
    "#     pool = conv_and_pooling(input_xs,CONV_SIZE,EMBEDDING_SIZE,1,CONV_DEEP)\n",
    "    \n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped = tf.reshape(pool,[-1,nodes])\n",
    "    \n",
    "    #the first fully connected layer\n",
    "    fc1_weights = get_weights_variable([nodes,FC_SIZE])\n",
    "    fc1_bias = tf.Variable(tf.constant(0.1,shape=[FC_SIZE]))\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_weights) + fc1_bias)\n",
    "    fc1 = tf.nn.dropout(fc1,keep_prob)\n",
    " \n",
    "    #the second fully connected layer(output layer)\n",
    "    fc2_weights = get_weights_variable([FC_SIZE,NUM_CLASSES])\n",
    "    fc2_bias = tf.Variable(tf.constant(0.1,shape=[NUM_CLASSES]))\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1,fc2_weights) + fc2_bias)\n",
    "    \n",
    "    return fc2\n",
    "\n",
    "def evaluation_op(predic_output):\n",
    "    #calculate TP，TN，FP，FN on test_data\n",
    "    predictions = tf.argmax(predic_output, 1)\n",
    "    actuals = tf.argmax(input_ys, 1)\n",
    "\n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    tn_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "            tf.logical_and(\\\n",
    "            tf.equal(actuals, ones_like_actuals),\\\n",
    "            tf.equal(predictions, ones_like_predictions)\\\n",
    "            ), \\\n",
    "            \"float\")\\\n",
    "        )\n",
    "\n",
    "    tp_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "            tf.logical_and(\\\n",
    "            tf.equal(actuals, zeros_like_actuals),\\\n",
    "            tf.equal(predictions, zeros_like_predictions)\\\n",
    "            ),\\\n",
    "            \"float\")\\\n",
    "        )\n",
    "\n",
    "    fn_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "          tf.logical_and(\\\n",
    "            tf.equal(actuals, zeros_like_actuals),\\\n",
    "            tf.equal(predictions, ones_like_predictions)\\\n",
    "          ),\\\n",
    "          \"float\")\\\n",
    "        )\n",
    "\n",
    "    fp_op = tf.reduce_sum(\\\n",
    "        tf.cast(\\\n",
    "          tf.logical_and(\\\n",
    "            tf.equal(actuals, ones_like_actuals),\\\n",
    "            tf.equal(predictions, zeros_like_predictions)\\\n",
    "          ),\\\n",
    "          \"float\")\\\n",
    "        )\n",
    "    return tp_op, tn_op,fp_op, fn_op\n",
    "\n",
    "\n",
    "\n",
    "def print_test_evaluation(tp,tn,fp,fn):  \n",
    "    tpr = float(tp)/(float(tp) + float(fn))\n",
    "    recall = tpr\n",
    "    print(\"Sensitivity/recall on the test data is :{}\".format(tpr)) \n",
    "\n",
    "    specifity = float(tn)/(float(tn) + float(fp))\n",
    "    print(\"specifity on the test data is :{}\".format(specifity)) \n",
    "\n",
    "    precision = float(tp)/(float(tp) + float(fp))\n",
    "    print(\"precision on the test data is :{}\".format(precision))\n",
    "\n",
    "    f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "    print(\"f1_score on the test data is :{}\".format(f1_score))\n",
    "\n",
    "    fpr = float(fp)/(float(tp) + float(fn))\n",
    "    print(\"fpr on the test data is :{}\".format(fpr))\n",
    "\n",
    "    mcc = ((float(tp) * float(tn)) - (float(fp) * float(fn))) /\\\n",
    "            math.sqrt((float(tp) + float(fp)) * (float(tp) + float(fn))*\\\n",
    "             (float(tn) + float(fp)) * (float(tn) + float(fn)))\n",
    "    print(\"mcc on the test data is :{}\".format(mcc))\n",
    "\n",
    "    accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "    print(\"accuracy on the test data is :{}\".format(accuracy))\n",
    "    \n",
    "def cnn_train():   \n",
    "#     calculate the loss entropy \n",
    "    cnn_output = cnn_inference()\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits\\\n",
    "                                (logits=cnn_output,labels=input_ys)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar(\"loss\",cross_entropy_mean)\n",
    "#     optimization\n",
    "    train_op = tf.train.AdamOptimizer(LR).minimize(cross_entropy_mean)  \n",
    "\n",
    "#     calculate the accuracy of the model\n",
    "    correct_prediction = tf.equal(tf.argmax(cnn_output,1), tf.argmax(input_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "    tf.summary.scalar(\"accuracy\",accuracy)\n",
    "#     evaluation on test_data\n",
    "    tp_op,tn_op,fp_op,fn_op = evaluation_op(cnn_output)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:  # run the session        \n",
    "        writer = tf.summary.FileWriter(\"logs/filter6\", sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(TRAINING_ITER): \n",
    "            start = (i * BATCH_SIZE)% dataset_size\n",
    "            end = min(start + BATCH_SIZE,dataset_size)\n",
    "            batch_xs=training_x_data[start:end]\n",
    "            batch_ys=training_y_data[start:end]\n",
    "\n",
    "            _,rs = sess.run([train_op,merged],feed_dict={input_xs:batch_xs,input_ys:batch_ys,\\\n",
    "                                         keep_prob:DROPOUT_KEEP_PROB})\n",
    "            writer.add_summary(rs,i)\n",
    "            \n",
    "#             print loss and accuracy during the training process\n",
    "            if(i%100==0):\n",
    "                print(\"The{}iteration:\".format(i))\n",
    "                print(\"The cross_entropy_mean为：\",end='')\n",
    "                print(sess.run(cross_entropy_mean,feed_dict={input_xs:batch_xs,\\\n",
    "                                input_ys:batch_ys,keep_prob:DROPOUT_KEEP_PROB}))\n",
    "\n",
    "                print(\"The accuracy on training data:\",end='')\n",
    "                print(sess.run(accuracy,feed_dict={input_xs:training_x_data,\\\n",
    "                                input_ys:training_y_data,keep_prob:1}))\n",
    "                print(\"The accuracy on test data:\",end='')\n",
    "                print(sess.run(accuracy,feed_dict={input_xs:test_x_data,\\\n",
    "                                           input_ys:test_y_data,keep_prob:1}))\n",
    "                print(\"==================\")\n",
    "                \n",
    "            saver.save(sess,'logs/filter6/model.ckpt')  \n",
    "                \n",
    "                \n",
    "        print(\"*********training finished********\")\n",
    "        \n",
    "#         evaluation on test data set\n",
    "\n",
    "        tp,tn,fp,fn = sess.run([tp_op, tn_op,fp_op, fn_op],\\\n",
    "                feed_dict={input_xs:test_x_data,\\\n",
    "                           input_ys:test_y_data,keep_prob:1})\n",
    "        print(\"tp:{},tn:{},fp:{},fn:{}\".format(tp,tn,fp,fn))\n",
    "    \n",
    "        print_test_evaluation(tp,tn,fp,fn)\n",
    "        \n",
    "        \n",
    "        \n",
    "def main(argv=None):\n",
    "    cnn_train()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
